{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa9f736-2c54-42f9-937f-2d54e41d127e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T13:03:18.446143+00:00",
          "start_time": "2023-05-27T13:03:02.459577+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "!pip3 install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip3 install jiwer\n",
        "!pip3 install ds-ctcdecoder\n",
        "!pip3 install levenshtein\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "052a98bf-88fd-4e43-bd5c-f60846af66fd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T13:27:11.844269+00:00",
          "start_time": "2023-05-27T13:26:50.998928+00:00"
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Pulling datasets in\n",
        "%ntbl pull datasets \"BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Train.csv\"\n",
        "\n",
        "%ntbl pull datasets \"BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Train.zip\"\n",
        "\n",
        "!unzip \"../datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Train.zip\" -d /tmp/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b21b83b-0db1-4ff4-8582-3cc2892ab705",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T14:16:40.748396+00:00",
          "start_time": "2023-05-27T14:16:40.592948+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# File paths\n",
        "MANIFEST_FILE_TRAIN = '../datasets/BrainHack TIL-23 Automatic Speech Recognition Advanced Hackathon/Train.csv'\n",
        "AUDIO_DIR_TRAIN = '/tmp/data/Train/'\n",
        "SAVED_MODEL_PATH = '/models/ASR50_beam_model.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "54138f77",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch and os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "MANIFEST_FILE_TRAIN = 'src/dataset/trainshort.csv'\n",
        "AUDIO_DIR_TRAIN = 'src/dataset/trainshort/trainshort/'\n",
        "SAVED_MODEL_PATH = 'models/ASR50_beam_model.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "8bd5c731",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'soundfile'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(torchaudio.get_audio_backend())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "a348a0cb-5d31-4f14-adf9-1060146d03b8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T13:46:55.409354+00:00",
          "start_time": "2023-05-27T13:46:55.247879+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Custom Speech Dataset class to load the dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "\n",
        "class CustomSpeechDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    \"\"\"\n",
        "    Custom torch dataset class to load the dataset \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, manifest_file: str, audio_dir: str, is_test_set: bool=False) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        manifest_file: the csv file that contains the filename of the audio, and also the annotation if is_test_set is set to False\n",
        "        audio_dir: the root directory of the audio datasets\n",
        "        is_test_set: the flag variable to switch between loading of the train and the test set. Train set loads the annotation whereas test set does not\n",
        "        \"\"\"\n",
        "\n",
        "        self.audio_dir = audio_dir\n",
        "        self.is_test_set = is_test_set\n",
        "\n",
        "        self.manifest = pd.read_csv(manifest_file)\n",
        "\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        \n",
        "        \"\"\"\n",
        "        To get the number of loaded audio files in the dataset\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.manifest)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index: int) -> Tuple[str, torch.Tensor]:\n",
        "\n",
        "        \"\"\"\n",
        "        To get the values required to do the training\n",
        "        \"\"\"\n",
        "\n",
        "        if torch.is_tensor(index):\n",
        "            index.tolist()\n",
        "            \n",
        "        audio_path = self._get_audio_path(index)\n",
        "        signal, sr = torchaudio.load(audio_path)\n",
        "        \n",
        "        if not self.is_test_set:\n",
        "            annotation = self._get_annotation(index)\n",
        "            return audio_path, signal, annotation\n",
        "        \n",
        "        return audio_path, signal\n",
        "    \n",
        "    \n",
        "    def _get_audio_path(self, index: int) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Helper function to retrieve the audio path from the csv manifest file\n",
        "        \"\"\"\n",
        "        \n",
        "        path = os.path.join(self.audio_dir, self.manifest.iloc[index]['path'])\n",
        "\n",
        "        return path\n",
        "    \n",
        "    \n",
        "    def _get_annotation(self, index: int) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Helper function to retrieve the annotation from the csv manifest file\n",
        "        \"\"\"\n",
        "\n",
        "        return self.manifest.iloc[index]['annotation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "67c9b94e-e00f-4660-a485-ff700ec6be3f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T13:52:47.077784+00:00",
          "start_time": "2023-05-27T13:52:40.800739+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Create custom dataset\n",
        "dataset = CustomSpeechDataset(\n",
        "        manifest_file=MANIFEST_FILE_TRAIN, \n",
        "        audio_dir=AUDIO_DIR_TRAIN, \n",
        "        is_test_set=False\n",
        "    )\n",
        "# train_dev_split\n",
        "train_proportion = int(0.8 * len(dataset))\n",
        "dataset_train = list(dataset)[:train_proportion]\n",
        "dataset_dev = list(dataset)[train_proportion:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "14cd4573-17ea-4b45-b03c-9af8ff22f6cc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T13:52:47.249802+00:00",
          "start_time": "2023-05-27T13:52:47.084237+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Transforms text by encoding the characters and decoding the integers corresponding to the characters\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class TextTransform:\n",
        "\n",
        "    \"\"\"\n",
        "    Map characters to integers and vice versa (encoding/decoding)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        char_map_str = \"\"\"\n",
        "            <SPACE> 0\n",
        "            A 1\n",
        "            B 2\n",
        "            C 3\n",
        "            D 4\n",
        "            E 5\n",
        "            F 6\n",
        "            G 7\n",
        "            H 8\n",
        "            I 9\n",
        "            J 10\n",
        "            K 11\n",
        "            L 12\n",
        "            M 13\n",
        "            N 14\n",
        "            O 15\n",
        "            P 16\n",
        "            Q 17\n",
        "            R 18\n",
        "            S 19\n",
        "            T 20\n",
        "            U 21\n",
        "            V 22\n",
        "            W 23\n",
        "            X 24\n",
        "            Y 25\n",
        "            Z 26\n",
        "        \"\"\"\n",
        "        \n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        \n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "\n",
        "        self.index_map[0] = ' '\n",
        "\n",
        "\n",
        "    def get_char_len(self) -> int:\n",
        "\n",
        "        \"\"\"\n",
        "        Gets the number of characters that are being encoded and decoded in the prediction\n",
        "        Returns:\n",
        "        --------\n",
        "            the number of characters defined in the __init__ char_map_str\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.char_map)\n",
        "    \n",
        "\n",
        "    def get_char_list(self) -> List[str]:\n",
        "\n",
        "        \"\"\"\n",
        "        Gets the list of characters that are being encoded and decoded in the prediction\n",
        "        \n",
        "        Returns:\n",
        "        -------\n",
        "            a list of characters defined in the __init__ char_map_str\n",
        "        \"\"\"\n",
        "\n",
        "        return list(self.index_map.values())\n",
        "    \n",
        "\n",
        "    def text_to_int(self, text: str) -> List[int]:\n",
        "\n",
        "        \"\"\"\n",
        "        Use a character map and convert text to an integer sequence \n",
        "        Returns:\n",
        "        -------\n",
        "            a list of the text encoded to an integer sequence \n",
        "        \"\"\"\n",
        "        \n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "\n",
        "        return int_sequence\n",
        "    \n",
        "\n",
        "    def int_to_text(self, labels) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Use a character map and convert integer labels to an text sequence \n",
        "        \n",
        "        Returns:\n",
        "        -------\n",
        "            the decoded transcription\n",
        "        \"\"\"\n",
        "        \n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "\n",
        "        return ''.join(string).replace('<SPACE>', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "8bab6c71-7734-4eeb-b803-acff834f823c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T13:51:18.277836+00:00",
          "start_time": "2023-05-27T13:51:18.111394+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Data preprocessing and transformation of the audio files into melspectrogram\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import random\n",
        "\n",
        "class DataProcessor:\n",
        "\n",
        "    \"\"\"\n",
        "    Transforms the audio waveform tensors into a melspectrogram\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    def _audio_transformation(self, is_train: bool=True):\n",
        "\n",
        "        return torch.nn.Sequential(\n",
        "                torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        "            ) if is_train else torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "    \n",
        "\n",
        "    def _pitch_shift_aug(self, waveform, sample_rate):\n",
        "        \"\"\"\n",
        "        Take in a waveform and return a waveform\n",
        "        \"\"\"\n",
        "        # Choose a random pitch shift factor between -4 and +4 semitones\n",
        "        pitch_shift_factor = random.uniform(-4.0, 4.0)\n",
        "\n",
        "        # Apply the pitch shift effect\n",
        "        pitch_transform = torchaudio.transforms.PitchShift(sample_rate=sample_rate, n_fft=1024, n_steps=pitch_shift_factor)\n",
        "        pitch_shifted_waveform = pitch_transform(waveform)\n",
        "        return pitch_shifted_waveform\n",
        "    \n",
        "    def _add_noise_aug(self, waveform):\n",
        "        # Add random noise to the soundwave tensor array\n",
        "        num_samples = waveform.shape[-1]\n",
        "        num_noise_samples = int(num_samples * 0.1) # add 10% of noise samples\n",
        "        noise_indices = torch.randint(low=0, high=num_samples, size=(num_noise_samples,))\n",
        "        noise_values = torch.randn(size=(num_noise_samples,)) # generate random noise values\n",
        "        waveform[0, noise_indices] += noise_values # add noise to the first channel of the waveform tensor array\n",
        "        return waveform\n",
        "            \n",
        "\n",
        "    def data_processing(self, data, data_type='train'):\n",
        "\n",
        "        \"\"\"\n",
        "        Process the audio data to retrieve the spectrograms that will be used for the training\n",
        "        \"\"\"\n",
        "\n",
        "        text_transform = TextTransform()\n",
        "        spectrograms = []\n",
        "        input_lengths = []\n",
        "        audio_path_list = []\n",
        "\n",
        "        audio_transforms = self._audio_transformation(is_train=True) if data_type == 'train' else self._audio_transformation(is_train=False)\n",
        "\n",
        "        if data_type != 'test':  \n",
        "            labels = []\n",
        "            label_lengths = []\n",
        "\n",
        "            for audio_path, waveform, utterance in data:\n",
        "                \n",
        "                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "                spectrograms.append(spec)\n",
        "                label = torch.Tensor(text_transform.text_to_int(utterance))\n",
        "                labels.append(label)\n",
        "                input_lengths.append(spec.shape[0]//2)\n",
        "                label_lengths.append(len(label))\n",
        "                if (random.uniform(-1.0, 1.0) == 0):\n",
        "                    augmented_spec = audio_transforms(self._pitch_shift_aug(waveform, 16)).squeeze(0).transpose(0, 1)\n",
        "                    spectrograms.append(augmented_spec)\n",
        "                    labels.append(label)\n",
        "                    input_lengths.append(spec.shape[0]//2)\n",
        "                    label_lengths.append(len(label))\n",
        "                    print('change pitch')\n",
        "                if (random.uniform(-1.0, 1.0) == 0):\n",
        "                    augmented_spec = audio_transforms(self._add_noise_aug(waveform)).squeeze(0).transpose(0, 1)\n",
        "                    spectrograms.append(augmented_spec)\n",
        "                    labels.append(label)\n",
        "                    input_lengths.append(spec.shape[0]//2)\n",
        "                    label_lengths.append(len(label))\n",
        "                    print('Add noise')\n",
        "\n",
        "            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "            return audio_path, spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "        else:\n",
        "            for audio_path, waveform in data:\n",
        "\n",
        "                spec = audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "                spectrograms.append(spec)\n",
        "                input_lengths.append(spec.shape[0]//2)\n",
        "                audio_path_list.append(audio_path)\n",
        "\n",
        "            spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "            return audio_path_list, spectrograms, input_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "id": "4a680bfc-172a-4291-9507-6b92e5950fb4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T14:09:06.542866+00:00",
          "start_time": "2023-05-27T14:09:06.378065+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# Decodes the logits into characters to form the final transciption using the greedy decoding approach\n",
        "\n",
        "import torch\n",
        "from typing import List\n",
        "from torchaudio.models.decoder import ctc_decoder\n",
        "\n",
        "class GreedyDecoder:\n",
        "\n",
        "    \"\"\"\n",
        "    Decodes the logits into characters to form the final transciption using the greedy decoding approach\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "\n",
        "    def decode(\n",
        "            self, \n",
        "            output: torch.Tensor, \n",
        "            labels: torch.Tensor=None, \n",
        "            label_lengths: List[int]=None, \n",
        "            collapse_repeated: bool=True, \n",
        "            is_test: bool=False\n",
        "        ):\n",
        "        \n",
        "        \"\"\"\n",
        "        Main method to call for the decoding of the text from the predicted logits\n",
        "        \"\"\"\n",
        "        \n",
        "        text_transform = TextTransform()\n",
        "        arg_maxes = torch.argmax(output, dim=2)\n",
        "        decodes = []\n",
        "\n",
        "        # refer to char_map_str in the TextTransform class -> only have index from 0 to 26, hence 27 represents the case where the character is decoded as blank (NOT <SPACE>)\n",
        "        decoded_blank_idx = text_transform.get_char_len()\n",
        "\n",
        "        if not is_test:\n",
        "            targets = []\n",
        "\n",
        "        for i, args in enumerate(arg_maxes):\n",
        "            decode = []\n",
        "\n",
        "            if not is_test:\n",
        "                targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\n",
        "            for j, char_idx in enumerate(args):\n",
        "                if char_idx != decoded_blank_idx:\n",
        "                    if collapse_repeated and j != 0 and char_idx == args[j-1]:\n",
        "                        continue\n",
        "                    decode.append(char_idx.item())\n",
        "            decodes.append(text_transform.int_to_text(decode))\n",
        "\n",
        "        return decodes, targets if not is_test else decodes\n",
        "    \n",
        "\n",
        "class BeamSearchDecoder:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def decode(self, \n",
        "            output: torch.Tensor, \n",
        "            labels: torch.Tensor=None, \n",
        "            label_lengths: List[int]=None, \n",
        "            collapse_repeated: bool=True, \n",
        "            is_test: bool=False,\n",
        "            beam_size=1500, \n",
        "            lm_weight=3.23, \n",
        "            word_score=-0.26, \n",
        "        ):\n",
        "        text_transform = TextTransform()\n",
        "        beam_search_decoder = ctc_decoder(\n",
        "                                lexicon=None,\n",
        "                                tokens=text_transform.get_char_list(),\n",
        "                                nbest=3,\n",
        "                                beam_size=beam_size,\n",
        "                                lm=None,\n",
        "                                lm_weight=lm_weight,\n",
        "                                lm_dict=None,\n",
        "                                word_score=word_score,\n",
        "                                blank_token=' ',\n",
        "                                sil_token=' ',\n",
        "                            )\n",
        "        arg_maxes = torch.argmax(output, dim=2)\n",
        "        if not is_test:\n",
        "            targets = []\n",
        "        decodes = []\n",
        "        result = beam_search_decoder(output.to(torch.float32))\n",
        "        \n",
        "        for i, args in enumerate(arg_maxes):\n",
        "            statement = \" \".join(result[i][0].words).strip()\n",
        "            decodes.append(statement.upper())\n",
        "            if not is_test:\n",
        "                targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        \n",
        "        return decodes, targets if not is_test else decodes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "5065c44e-1a18-438c-9ce3-0dc1b3a84709",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T14:09:21.180149+00:00",
          "start_time": "2023-05-27T14:09:21.010837+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# The helper class for the training loop to do model training\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from jiwer import wer, cer\n",
        "import Levenshtein as leven\n",
        "\n",
        "class IterMeter(object):\n",
        "\n",
        "    \"\"\"\n",
        "    Keeps track of the total iterations during the training and validation loop\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "        self.val = 0\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "    \n",
        "\n",
        "class TrainingLoop:\n",
        "\n",
        "    \"\"\"\n",
        "    The main class to set up the training loop to train the model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "    \n",
        "\n",
        "    def train(self, model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Training Loop\n",
        "        \"\"\"\n",
        "        text_transform = TextTransform()\n",
        "        beam_search = BeamSearchDecoder()\n",
        "\n",
        "        model.train()\n",
        "        data_len = len(train_loader.dataset)\n",
        "        all_train_decoded = []\n",
        "        all_train_actual = []\n",
        "        \n",
        "        for batch_idx, _data in enumerate(train_loader):\n",
        "            audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            iter_meter.step()\n",
        "            if batch_idx > (data_len - 5):\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    decoded = beam_search.decode(spectrograms)\n",
        "                    for j in range(0, len(decoded)):\n",
        "                        actual = text_transform.int_to_text(labels.cpu().numpy()[j][:label_lengths[j]].tolist())\n",
        "                        all_train_decoded.append(decoded[j])\n",
        "                        all_train_actual.append(actual)\n",
        "                        train_levenshtein += leven.distance(decoded[j], actual)\n",
        "                        len_levenshtein += label_lengths[j]\n",
        "            \n",
        "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(spectrograms), data_len,\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "    def dev(self, model, device, dev_loader, criterion, scheduler, epoch, iter_meter) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Validation Loop\n",
        "        \"\"\"\n",
        "        \n",
        "        print('\\nevaluating...')\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        test_cer, test_wer = [], []\n",
        "        beam_search = BeamSearchDecoder()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(dev_loader):\n",
        "                audio_path, spectrograms, labels, input_lengths, label_lengths = _data \n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                val_loss += loss.item() / len(dev_loader)\n",
        "\n",
        "                # decoded_preds, decoded_targets = model.beam_search_with_lm(output.transpose(0, 1), labels=labels, label_lengths=label_lengths, is_test=False)\n",
        "                decoded_preds, decoded_targets = beam_search.decode(output.transpose(0, 1), labels=labels, label_lengths=label_lengths, is_test=False)\n",
        "\n",
        "                print(decoded_preds)\n",
        "                print(decoded_targets)\n",
        "                \n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "        avg_cer = sum(test_cer)/len(test_cer)\n",
        "        avg_wer = sum(test_wer)/len(test_wer)\n",
        "        \n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print('Dev set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(val_loss, avg_cer, avg_wer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "5483da0b-92f7-4c2d-85d7-88d40c2b1508",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T14:07:40.627156+00:00",
          "start_time": "2023-05-27T14:07:40.451892+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# building the model with adaption of deepspeech2 -> https://arxiv.org/abs/1512.02595\n",
        "\n",
        "# code adapted from https://towardsdatascience.com/customer-case-study-building-an-end-to-end-speech-recognition-model-in-pytorch-with-assemblyai-473030e47c7c\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class CNNLayerNorm(torch.nn.Module):\n",
        "    \n",
        "    \"\"\"\n",
        "    Layer normalization built for CNNs input\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_feats: int) -> None:\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "\n",
        "        self.layer_norm = torch.nn.LayerNorm(n_feats)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Input x of dimension -> (batch, channel, feature, time)\n",
        "        \"\"\"\n",
        "        \n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel: int, stride: int, dropout: float, n_feats: int) -> None:\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = torch.nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = torch.nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = torch.nn.Dropout(dropout)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "        self.gelu = torch.nn.GELU()\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \n",
        "        \"\"\"\n",
        "        Model building for the Residual CNN layers\n",
        "        \n",
        "        Input x of dimension -> (batch, channel, feature, time)\n",
        "        \"\"\"\n",
        "\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    The Bidirectional GRU composite code block which will be used in the main SpeechRecognitionModel class\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, rnn_dim: int, hidden_size: int, dropout: int, batch_first: int) -> None:\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = torch.nn.GRU(\n",
        "            input_size=rnn_dim, \n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1, \n",
        "            batch_first=batch_first, \n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.layer_norm = torch.nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.gelu = torch.nn.GELU()\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "        Transformation of the layers in the Bidirectional GRU block\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    The main ASR Model that the main code will interact with\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1) -> None:\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        \n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = torch.nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = torch.nn.Sequential(*[\n",
        "            ResidualCNN(\n",
        "                in_channels=32, \n",
        "                out_channels=32, \n",
        "                kernel=3, \n",
        "                stride=1, \n",
        "                dropout=dropout, \n",
        "                n_feats=n_feats\n",
        "            ) for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = torch.nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = torch.nn.Sequential(*[\n",
        "            BidirectionalGRU(\n",
        "                rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                hidden_size=rnn_dim, \n",
        "                dropout=dropout, \n",
        "                batch_first=i==0\n",
        "            ) for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "        Transformation of the layers in the ASR model block\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "ec284e85-f159-47c1-8a01-8e41ed09061e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-27T14:10:30.767817+00:00",
          "start_time": "2023-05-27T14:10:30.604519+00:00"
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": [
        "# main function\n",
        "def main(hparams, train_dataset, dev_dataset, saved_model_path) -> None:\n",
        "\n",
        "    \"\"\"\n",
        "    The main method to call to do model training\n",
        "    \"\"\" \n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(SEED)\n",
        "    \n",
        "    data_processor = DataProcessor()\n",
        "    iter_meter = IterMeter()\n",
        "    text_transform = TextTransform()\n",
        "    trainer = TrainingLoop()\n",
        "    \n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=hparams['batch_size'],\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda x: data_processor.data_processing(x, 'train'),\n",
        "        **kwargs\n",
        "    )\n",
        "    \n",
        "    dev_loader = torch.utils.data.DataLoader(\n",
        "        dataset=dev_dataset,\n",
        "        batch_size=hparams['batch_size'],\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: data_processor.data_processing(x, 'dev'),\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], \n",
        "        hparams['n_rnn_layers'], \n",
        "        hparams['rnn_dim'],\n",
        "        hparams['n_class'], \n",
        "        hparams['n_feats'], \n",
        "        hparams['stride'], \n",
        "        hparams['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    loss_fn = torch.nn.CTCLoss(blank=text_transform.get_char_len()).to(device)\n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=3, verbose=True, factor=0.05)\n",
        "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], steps_per_epoch=int(len(train_loader)), epochs=hparams['epochs'], anneal_strategy='linear')\n",
        "    \n",
        "    for epoch in range(1, hparams['epochs'] + 1):\n",
        "        trainer.train(model, device, train_loader, loss_fn, optimizer, scheduler, epoch, iter_meter)\n",
        "        trainer.dev(model, device, dev_loader, loss_fn, scheduler, epoch, iter_meter)\n",
        "        \n",
        "    # save the trained model\n",
        "    torch.save(model.state_dict(), saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "4f5e19b7-4566-4479-a044-9144d2df5856",
      "metadata": {
        "ExecuteTime": null,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num Model Parameters 23704860\n",
            "data length:  80\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\cheng yang\\Anaconda3\\lib\\site-packages\\torchaudio\\functional\\functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "Train Epoch: 1 [0/80 (0%)]\tLoss: 7.677923\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "\n",
            "evaluating...\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['THE BUS ARRIVES EVERY TWO HOURS AT THIS STATION', 'THE PROJECT WILL TAKE TWO MONTHS TO COMPLETE', 'SHE BOUGHT THREE NEW DRESSES FOR THE WEDDING', 'THEY ONLY HAD SEVEN MINUTES TO GET READY BEFORE LEAVING', 'THERE IS ONLY ONE RULE AND THAT IS NO CHEATING', 'HE CAUGHT EIGHT FISH ON THEIR FISHING TRIP', 'THERE CAN ONLY BE ONE WINNER FOR THE CONTEST', 'THEY STAYED IN THE HOTEL ROOM FOR SEVEN NIGHTS', 'THEY HAD TO CLIMB FOUR FLIGHTS OF STAIRS TO REACH THE TOP', 'SHE DONATED NINE BAGS OF CLOTHES TO CHARITY', 'SHE RECEIVED NINE BIRTHDAY CARDS IN THE MAIL', 'SHE SPENT SIX WEEKS VOLUNTEERING AT AN ANIMAL SHELTER', 'THE SEVEN SAMURAI IS A FAMOUS JAPANESE FILM', 'THEY FOUND SEVEN DIFFERENT SPECIES OF FLOWERS IN THE GARDEN', 'SHE WON SIX AWARDS FOR HER ARTWORK', 'THE BUS ARRIVES EVERY TWO HOURS AT THIS STATION']\n",
            "['', '', '', '']\n",
            "['SHE IS ONE OF THE MOST GENEROUS PEOPLE I HAVE EVER MET', 'WE ONLY HAVE ONE LIFE TO LIVE SO MAKE IT COUNT', 'HE HAD TO COMPLETE FOUR LAPS AROUND THE TRACK TO FINISH THE RACE', 'THEY FOUND A NINE LETTER WORD IN THE CROSSWORD PUZZLE']\n",
            "Dev set: Average loss: 3.1935, Average CER: 1.000000 Average WER: 1.0000\n",
            "\n",
            "data length:  80\n",
            "6\n",
            "Train Epoch: 2 [0/80 (0%)]\tLoss: 3.225823\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "\n",
            "evaluating...\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "['THE BUS ARRIVES EVERY TWO HOURS AT THIS STATION', 'THE PROJECT WILL TAKE TWO MONTHS TO COMPLETE', 'SHE BOUGHT THREE NEW DRESSES FOR THE WEDDING', 'THEY ONLY HAD SEVEN MINUTES TO GET READY BEFORE LEAVING', 'THERE IS ONLY ONE RULE AND THAT IS NO CHEATING', 'HE CAUGHT EIGHT FISH ON THEIR FISHING TRIP', 'THERE CAN ONLY BE ONE WINNER FOR THE CONTEST', 'THEY STAYED IN THE HOTEL ROOM FOR SEVEN NIGHTS', 'THEY HAD TO CLIMB FOUR FLIGHTS OF STAIRS TO REACH THE TOP', 'SHE DONATED NINE BAGS OF CLOTHES TO CHARITY', 'SHE RECEIVED NINE BIRTHDAY CARDS IN THE MAIL', 'SHE SPENT SIX WEEKS VOLUNTEERING AT AN ANIMAL SHELTER', 'THE SEVEN SAMURAI IS A FAMOUS JAPANESE FILM', 'THEY FOUND SEVEN DIFFERENT SPECIES OF FLOWERS IN THE GARDEN', 'SHE WON SIX AWARDS FOR HER ARTWORK', 'THE BUS ARRIVES EVERY TWO HOURS AT THIS STATION']\n",
            "['', '', '', '']\n",
            "['SHE IS ONE OF THE MOST GENEROUS PEOPLE I HAVE EVER MET', 'WE ONLY HAVE ONE LIFE TO LIVE SO MAKE IT COUNT', 'HE HAD TO COMPLETE FOUR LAPS AROUND THE TRACK TO FINISH THE RACE', 'THEY FOUND A NINE LETTER WORD IN THE CROSSWORD PUZZLE']\n",
            "Dev set: Average loss: 2.9019, Average CER: 1.000000 Average WER: 1.0000\n",
            "\n",
            "Time taken for training: 0.049466379152403935 hrs\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "\n",
        "# setting the random seed for reproducibility\n",
        "SEED = 2022\n",
        "\n",
        "# calling the model\n",
        "hparams = {\n",
        "            \"n_cnn_layers\": 7,\n",
        "            \"n_rnn_layers\": 5,\n",
        "            \"rnn_dim\": 512,\n",
        "            \"n_class\": 28, # 26 alphabets in caps + <SPACE> + blanks\n",
        "            \"n_feats\": 128,\n",
        "            \"stride\": 2,\n",
        "            \"dropout\": 0.2,\n",
        "            \"learning_rate\": 5e-4,\n",
        "            \"batch_size\": 16,\n",
        "            \"epochs\": 50\n",
        "      }\n",
        "start_time = time()\n",
        "\n",
        "# start training the model\n",
        "main(\n",
        "    hparams=hparams, \n",
        "    train_dataset=dataset_train, \n",
        "    dev_dataset=dataset_dev, \n",
        "    saved_model_path=SAVED_MODEL_PATH\n",
        ")\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "print(f\"Time taken for training: {(end_time-start_time)/(60*60)} hrs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7321996-50d2-4f95-8977-65b127573831",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee67b2a0-5873-453b-8e68-576dfeefb49b",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1998435-4c1e-4db6-987a-c8adfc7aabdb",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "noteable": {
      "last_delta_id": "0a209847-bef0-4641-bf83-9679943e276a",
      "last_transaction_id": "31bbd084-681b-4fd4-bb38-66477089b23e"
    },
    "nteract": {
      "version": "noteable@2.9.0"
    },
    "selected_hardware_size": "xlarge"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
